{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "819e09b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of downloadable files: 1000\n",
      "Starting download of 10 files...\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000616448-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000618496-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000620544-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000622592-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000624640-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000626688-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000628736-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000630784-2048.gwf, already exists.\n",
      "Skipping ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000632832-2048.gwf, already exists.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pycbc.waveform import get_td_waveform\n",
    "from gwpy.timeseries import TimeSeries\n",
    "from scipy.signal import spectrogram, welch, butter, filtfilt, stft\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy.signal.windows import tukey\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from astropy.cosmology import z_at_value, Planck18 as cosmo\n",
    "from astropy import units as u\n",
    "\n",
    "\n",
    "def download_ET_noise_files(n_downloads=5, file_list='./MDC1_v2_noise_E1.txt', outdir=\"data\"):\n",
    "    '''\n",
    "    Download file from url to outdir with a progress bar\n",
    "\n",
    "    for Downloading ET noise data files \n",
    "    '''\n",
    "    \n",
    "\n",
    "    with open(file_list) as file:\n",
    "        filenames = [line.rstrip() for line in file][300:]\n",
    "    print(f'Number of downloadable files: {len(filenames)}')\n",
    "    print(f'Starting download of {n_downloads} files...')\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    local_filenames = []\n",
    "    for filename in filenames[:n_downloads]:\n",
    "        local_filename = os.path.join(outdir, filename.split('/')[-1])\n",
    "        \n",
    "        # Skip if file already exists\n",
    "        if os.path.exists(local_filename):\n",
    "            local_filenames.append(local_filename)\n",
    "            print(f\"Skipping {local_filename}, already exists.\")\n",
    "            continue\n",
    "        \n",
    "        url = f'http://et-origin.cism.ucl.ac.be/{filename}'\n",
    "\n",
    "        # Stream download so we don’t load the whole file into memory\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get(\"content-length\", 0))\n",
    "            block_size = 1024  # 1 KB\n",
    "            progress = tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=local_filename)\n",
    "            \n",
    "            with open(local_filename, \"wb\") as f:\n",
    "                for chunk in r.iter_content(block_size):\n",
    "                    f.write(chunk)\n",
    "                    progress.update(len(chunk))\n",
    "            progress.close()\n",
    "        local_filenames.append(local_filename)\n",
    "        \n",
    "    return local_filenames\n",
    "\n",
    "### DOWNLOAD ET NOISE FILES ###\n",
    "raw_data_dir = './ET_noise/raw_waveforms/'\n",
    "ET_noise_file_locations = download_ET_noise_files(n_downloads=10, outdir=raw_data_dir) # you will need to download more than 10, this is just to demonstrate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7f34f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000616448-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000618496-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000620544-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000622592-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000624640-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000626688-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000628736-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000630784-2048.gwf',\n",
       " './ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000632832-2048.gwf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_noise_file_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3a7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 132.23seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000616448-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000616448-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 139.90seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000618496-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000618496-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 140.74seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000620544-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000620544-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 140.83seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000622592-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000622592-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 140.39seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000624640-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000624640-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 137.39seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000626688-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000626688-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 141.09seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000628736-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000628736-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 141.80seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000630784-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000630784-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 141.14seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000632832-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000632832-2048.gwf: 100%|██████████| 1023/1023 [00:07<00:00, 141.42seg/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# whitening\n",
    "def whiten(data, f_psd, psd, fs):\n",
    "    psd = np.maximum(psd, 1e-20)\n",
    "    freqs = np.fft.fftfreq(len(data), d=1/fs)\n",
    "    psd_interp = np.interp(freqs, f_psd, psd)\n",
    "    white_fft = fft(data) / np.sqrt(psd_interp)\n",
    "    return np.real(ifft(white_fft))\n",
    "\n",
    "\n",
    "def process_file(data_file, out_dir, train_test='train'):\n",
    "\n",
    "    '''Process a single ET noise data file: read, cut into 2s segments, whiten the noise, and save as tensors.'''\n",
    "\n",
    "    print(f'Processing: {data_file}')\n",
    "    try:\n",
    "        strain = TimeSeries.read(data_file, channel=channel)\n",
    "        f_psd, psd = welch(strain.value, fs=sample_rate, nperseg=sample_rate*4)\n",
    "\n",
    "        num_segments = int((strain.times.value[-1] - strain.times.value[0]) // segment_duration)\n",
    "        segments = []\n",
    "\n",
    "        os.makedirs(f'{out_dir}/{train_test}/', exist_ok=True)\n",
    "        #for i in range(num_segments):\n",
    "        for i in tqdm(range(num_segments), desc=f'Processing {data_file}', unit='seg'):\n",
    "\n",
    "            start = strain.times.value[0] + i * segment_duration\n",
    "            end = start + segment_duration\n",
    "            if end > strain.times.value[-1]:\n",
    "                break\n",
    "\n",
    "            segment = strain.crop(start, end).value\n",
    "            segment = whiten(segment, f_psd, psd, sample_rate)\n",
    "            tensor = torch.tensor(segment, dtype=torch.float32).unsqueeze(0)\n",
    "            segments.append(tensor)\n",
    "\n",
    "        batch_tensor = torch.cat(segments, dim=0)\n",
    "        torch.save(batch_tensor, f'{out_dir}/{train_test}/' + data_file.split('/')[-1][:-4] + '.pt')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {data_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "segmented_data_dir = './ET_noise/segmented_data'\n",
    "channel = 'E1:STRAIN'\n",
    "sample_rate = 8192\n",
    "segment_duration = 2\n",
    "segment_length = sample_rate * segment_duration\n",
    "\n",
    "for file_loc in ET_noise_file_locations[:8]:\n",
    "    process_file(file_loc, out_dir=segmented_data_dir, train_test='train')\n",
    "\n",
    "for file_loc in ET_noise_file_locations[8:]:\n",
    "    process_file(file_loc, out_dir=segmented_data_dir, train_test='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a28b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf: 100%|██████████| 1023/1023 [03:55<00:00,  4.34seg/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf: 100%|██████████| 1023/1023 [03:55<00:00,  4.34seg/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000616448-2048.gwf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000614400-2048.gwf: 100%|██████████| 1023/1023 [03:55<00:00,  4.34seg/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000616448-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000616448-2048.gwf: 100%|██████████| 1023/1023 [03:59<00:00,  4.28seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000618496-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000618496-2048.gwf: 100%|██████████| 1023/1023 [03:53<00:00,  4.37seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000620544-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000620544-2048.gwf: 100%|██████████| 1023/1023 [03:53<00:00,  4.38seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000622592-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000622592-2048.gwf: 100%|██████████| 1023/1023 [03:57<00:00,  4.31seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000624640-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000624640-2048.gwf: 100%|██████████| 1023/1023 [03:52<00:00,  4.40seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000626688-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000626688-2048.gwf: 100%|██████████| 1023/1023 [03:55<00:00,  4.35seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000628736-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000628736-2048.gwf: 100%|██████████| 1023/1023 [03:55<00:00,  4.35seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000630784-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000630784-2048.gwf: 100%|██████████| 1023/1023 [03:52<00:00,  4.39seg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000632832-2048.gwf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./ET_noise/raw_waveforms/E-E1_STRAIN_NOISE-1000632832-2048.gwf: 100%|██████████| 1023/1023 [03:55<00:00,  4.35seg/s]\n"
     ]
    }
   ],
   "source": [
    "def inject_signal_into_noise(data_file, mass_range, distance_range, out_dir, train_test='train', channel = 'E1:STRAIN'):\n",
    "    ''' Process raw waveforms into spectrograms with injected signals '''\n",
    "\n",
    "    print(f'Processing: {data_file}')\n",
    "    try:\n",
    "        strain = TimeSeries.read(data_file, channel=channel)\n",
    "        f_psd, psd = welch(strain.value, fs=sample_rate, nperseg=sample_rate*4)\n",
    "\n",
    "        num_segments = int((strain.times.value[-1] - strain.times.value[0]) // segment_duration)\n",
    "        segments = []\n",
    "        injected_segments = []\n",
    "\n",
    "        #for i in range(num_segments):\n",
    "        for i in tqdm(range(num_segments), desc=f'Processing {data_file}', unit='seg'):\n",
    "\n",
    "            start = strain.times.value[0] + i * segment_duration\n",
    "            end = start + segment_duration\n",
    "            if end > strain.times.value[-1]:\n",
    "                break\n",
    "\n",
    "            m1 = np.random.uniform(mass_range[0], mass_range[1])\n",
    "            m2 = np.random.uniform(mass_range[0], m1)\n",
    "            distance = np.random.uniform(distance_range[0], distance_range[1])\n",
    "\n",
    "            lumi_distance = distance * u.Gpc\n",
    "            z = z_at_value(cosmo.luminosity_distance, lumi_distance)\n",
    "            m1_det = m1 * (1 + z)\n",
    "            m2_det = m2 * (1 + z)\n",
    "\n",
    "            segment = strain.crop(start, end).value\n",
    "            injected_segment = segment.copy()\n",
    "            hp, _ = get_td_waveform(\n",
    "                    approximant=\"IMRPhenomHM\",\n",
    "                    mass1=m1,#m1_det, \n",
    "                    mass2=m2,#m2_det,\n",
    "                    delta_t=1 / sample_rate,\n",
    "                    f_lower=3,\n",
    "                    distance=lumi_distance.to(u.Mpc).value,\n",
    "                    spin1z=0.0,\n",
    "                    spin2z=0.0,\n",
    "                    eccentricity=0.0,\n",
    "                    inclination=0.0\n",
    "                )\n",
    "\n",
    "            gw_tensor = torch.tensor(hp.numpy(), dtype=torch.float32)\n",
    "            peak = torch.argmax(gw_tensor).item()\n",
    "            new_peak = np.random.randint(int(0.2 * segment_length), int(0.8 * segment_length))\n",
    "            shift = new_peak - peak\n",
    "            gw_tensor = torch.roll(gw_tensor, shifts=shift)\n",
    "\n",
    "            start_idx = max(0, (new_peak - segment_length) // 2)\n",
    "            end_idx = start_idx + segment_length\n",
    "            gw_tensor = gw_tensor[start_idx:end_idx]\n",
    "\n",
    "            if len(gw_tensor) < segment_length:\n",
    "                gw_tensor = F.pad(gw_tensor, (0, segment_length - len(gw_tensor)))\n",
    "\n",
    "            #taper = tukey(segment_length, alpha=0.2)\n",
    "            gw_tapered = gw_tensor.numpy() #* taper\n",
    "\n",
    "            injected_segment += gw_tapered\n",
    "\n",
    "            injected_segment = whiten(injected_segment, f_psd, psd, sample_rate)\n",
    "            injected_tensor = torch.tensor(injected_segment, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            injected_segments.append(injected_tensor)\n",
    "\n",
    "        injected_batch_tensor = torch.cat(injected_segments, dim=0)\n",
    "        os.makedirs(out_dir + f'/{train_test}/', exist_ok=True)\n",
    "        torch.save(injected_batch_tensor, out_dir + f'/{train_test}/' + data_file.split('/')[-1][:-4] + '.pt')\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {data_file}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "channel = 'E1:STRAIN'\n",
    "sample_rate = 8192\n",
    "segment_duration = 2\n",
    "segment_length = sample_rate * segment_duration\n",
    "mass_range = [100, 200] # masses in solar mass\n",
    "distance_range = [5, 20] # distance in Gpc\n",
    "injected_data_dir = f'./ET_noise/injected_data/mass_{mass_range[0]}_{mass_range[1]}__distance_{distance_range[0]}_{distance_range[1]}Gpc'\n",
    "\n",
    "for file_loc in ET_noise_file_locations[:8]:\n",
    "    inject_signal_into_noise(file_loc, mass_range=mass_range, distance_range=distance_range, out_dir=injected_data_dir, train_test='train')\n",
    "\n",
    "for file_loc in ET_noise_file_locations[8:]:\n",
    "    inject_signal_into_noise(file_loc, mass_range=mass_range, distance_range=distance_range, out_dir=injected_data_dir, train_test='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98f6ca",
   "metadata": {},
   "source": [
    "### Load in processed data and train model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cf6674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectrogram(waveform, nperseg=512, noverlap=256, sample_rate=8192):\n",
    "    f, t, Zxx = stft(waveform, nperseg=nperseg, noverlap=noverlap, fs=sample_rate)\n",
    "    spec = np.abs(Zxx)\n",
    "    if spec.shape[1] > 4:\n",
    "        spec = spec[:, 1:-1]\n",
    "    return spec\n",
    "\n",
    "def preprocess_waveforms_to_spectrograms(waveforms, sample_rate=8192, nperseg=512, noverlap=256, norm=True):\n",
    "    processed = []\n",
    "    for waveform in waveforms:\n",
    "        spectrogram = compute_spectrogram(waveform.numpy(), sample_rate=sample_rate)\n",
    "        if norm:\n",
    "            min_val, max_val = np.min(spectrogram), np.max(spectrogram)\n",
    "            spectrogram = (spectrogram - min_val) / (max_val - min_val)\n",
    "\n",
    "        processed.append(spectrogram)\n",
    "    return torch.tensor(np.array(processed), dtype=torch.float32)\n",
    "\n",
    "def load_all_from_folder(folder_path):\n",
    "    all_waveforms = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pt'):\n",
    "            data = torch.load(os.path.join(folder_path, filename))\n",
    "            waveforms = data.get('waveforms', data) if isinstance(data, dict) else data\n",
    "            all_waveforms.append(waveforms)\n",
    "    return torch.cat(all_waveforms)\n",
    "\n",
    "class DeeperAutoencoderWithBottleneck(nn.Module):\n",
    "    def __init__(self, d=0.08):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.drop1 = nn.Dropout(d)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.drop2 = nn.Dropout(d)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.drop3 = nn.Dropout(d)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_enc = nn.Linear(128 * 32 * 3, 1024)\n",
    "        self.fc_dec = nn.Linear(1024, 128 * 32 * 3)\n",
    "        self.unflatten= nn.Unflatten(1, (128, 32, 3))\n",
    "        \n",
    "        self.unpool1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.drop4 = nn.Dropout(d)\n",
    "\n",
    "        self.unpool2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.drop5 = nn.Dropout(d)\n",
    "\n",
    "        self.unpool3 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        orig = x.size()\n",
    "        x = self.conv1(x); x = self.bn1(x); x = self.drop1(x); x, i1 = self.pool1(x); s1 = x.size()\n",
    "        x = self.conv2(x); x = self.bn2(x); x = self.drop2(x); x, i2 = self.pool2(x); s2 = x.size()\n",
    "        x = self.conv3(x); x = self.bn3(x); x = self.drop3(x); x, i3 = self.pool3(x); s3 = x.size()\n",
    "        x = self.unpool1(x, i3, output_size=s2); x = self.deconv1(x); x = self.bn4(x); x = self.drop4(x)\n",
    "        x = self.unpool2(x, i2, output_size=s1); x = self.deconv2(x); x = self.bn5(x); x = self.drop5(x)\n",
    "        x = self.unpool3(x, i1, output_size=orig); x = self.deconv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aedb8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 5728 training samples and 2456 validation samples \n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Load and Prepare Data\n",
    "# --------------------------\n",
    "noise_folder = \"./ET_noise/segmented_data/train/\" # path to processed ET noise spectrograms\n",
    "raw_waveforms = torch.cat([torch.load(os.path.join(noise_folder, filename)) for filename in os.listdir(noise_folder)])\n",
    "spectrograms = preprocess_waveforms_to_spectrograms(raw_waveforms, sample_rate=8192, nperseg=512, noverlap=256)\n",
    "spectrograms = torch.nn.functional.interpolate(spectrograms.unsqueeze(1), size=(256, 31))\n",
    "\n",
    "train_data, val_data = train_test_split(spectrograms, test_size=0.3, random_state=42)\n",
    "print(f\"Training with {train_data.shape[0]} training samples and {val_data.shape[0]} validation samples \")\n",
    "train_loader = DataLoader(TensorDataset(train_data), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_data), batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeeperAutoencoderWithBottleneck(d=0.08).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.005\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, threshold=0.005\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5593c1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m y = model(x)\n\u001b[32m     17\u001b[39m loss = criterion(y, x)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m optimizer.step()\n\u001b[32m     20\u001b[39m epoch_train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/gravWaves/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/gravWaves/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/gravWaves/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses = [], []\n",
    "train_errors_all, val_errors_all = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss, epoch_train_errors = 0.0, []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = model(x)\n",
    "        loss = criterion(y, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        epoch_train_errors.extend(((y - x)**2).mean(dim=[1, 2, 3]).detach().cpu().numpy())\n",
    "\n",
    "    train_losses.append(epoch_train_loss / len(train_loader))\n",
    "    train_errors_all.extend(epoch_train_errors)\n",
    "\n",
    "    model.eval()\n",
    "    epoch_val_loss, epoch_val_errors = 0.0, []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch[0].to(device)\n",
    "            y = model(x)\n",
    "            loss = criterion(y, x)\n",
    "            epoch_val_loss += loss.item()\n",
    "            epoch_val_errors.extend(((y - x)**2).mean(dim=[1, 2, 3]).cpu().numpy())\n",
    "\n",
    "    val_losses.append(epoch_val_loss / len(val_loader))\n",
    "    val_errors_all.extend(epoch_val_errors)\n",
    "    scheduler.step(epoch_val_loss)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}\")\n",
    "\n",
    "\n",
    "# Save Model\n",
    "model_path = \"./models/v3.0__unsupervised__ET_model.pt\" #path to where the model will be saved\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# --------------------------\n",
    "# Plot Losses\n",
    "# --------------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88e63417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 4092 training samples and 4092 validation samples \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m anomaly_x = batch_anomaly[\u001b[32m0\u001b[39m].to(device)          \u001b[38;5;66;03m# weakly supervised inputs\u001b[39;00m\n\u001b[32m     77\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m loss, noise_loss, anomaly_loss = \u001b[43mweakly_supervised_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manomaly_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m loss.backward()\n\u001b[32m     80\u001b[39m optimizer.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mweakly_supervised_loss\u001b[39m\u001b[34m(model, x_norm, x_anom, margin)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweakly_supervised_loss\u001b[39m(model, x_norm, x_anom, margin=\u001b[32m0.05\u001b[39m):\n\u001b[32m     44\u001b[39m     recon_norm = model(x_norm)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     recon_anom = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_anom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     L_norm = mse_loss(recon_norm, x_norm)\n\u001b[32m     48\u001b[39m     L_anom = mse_loss(recon_anom, x_anom)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/gravWaves/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/gravWaves/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mDeeperAutoencoderWithBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m x = \u001b[38;5;28mself\u001b[39m.unpool1(x, i3, output_size=s2); x = \u001b[38;5;28mself\u001b[39m.deconv1(x); x = \u001b[38;5;28mself\u001b[39m.bn4(x); x = \u001b[38;5;28mself\u001b[39m.drop4(x)\n\u001b[32m     70\u001b[39m x = \u001b[38;5;28mself\u001b[39m.unpool2(x, i2, output_size=s1); x = \u001b[38;5;28mself\u001b[39m.deconv2(x); x = \u001b[38;5;28mself\u001b[39m.bn5(x); x = \u001b[38;5;28mself\u001b[39m.drop5(x)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m x = \u001b[38;5;28mself\u001b[39m.unpool3(x, i1, output_size=orig); x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/gravWaves/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/gravWaves/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/gravWaves/lib/python3.13/site-packages/torch/nn/modules/conv.py:1162\u001b[39m, in \u001b[36mConvTranspose2d.forward\u001b[39m\u001b[34m(self, input, output_size)\u001b[39m\n\u001b[32m   1151\u001b[39m num_spatial_dims = \u001b[32m2\u001b[39m\n\u001b[32m   1152\u001b[39m output_padding = \u001b[38;5;28mself\u001b[39m._output_padding(\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1154\u001b[39m     output_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1159\u001b[39m     \u001b[38;5;28mself\u001b[39m.dilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1160\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Load and Prepare Data\n",
    "# --------------------------\n",
    "noise_folder = \"./ET_noise/segmented_data/train/\" # path to processed ET noise spectrograms\n",
    "raw_waveforms = torch.cat([torch.load(os.path.join(noise_folder, filename)) for filename in os.listdir(noise_folder)])\n",
    "spectrograms = preprocess_waveforms_to_spectrograms(raw_waveforms)\n",
    "\n",
    "noise_train_data, noise_val_data = train_test_split(spectrograms, test_size=0.5, random_state=42)\n",
    "\n",
    "noise_train_spectrograms = torch.nn.functional.interpolate(noise_train_data.unsqueeze(1), size=(256, 31))\n",
    "noise_train_loader = DataLoader(TensorDataset(noise_train_spectrograms), batch_size=64, shuffle=True)\n",
    "\n",
    "noise_val_spectrograms = torch.nn.functional.interpolate(noise_val_data.unsqueeze(1), size=(256, 31))\n",
    "noise_val_loader = DataLoader(TensorDataset(noise_val_spectrograms), batch_size=64, shuffle=False)\n",
    "\n",
    "anomaly_folder = \"./ET_noise/injected_data/mass_100_200__distance_5_20Gpc/train/\" #noFrameConversion__m_50_100__5_20Gpc/\" # path to processed ET noise spectrograms\n",
    "anomaly_waveforms = torch.cat([torch.load(os.path.join(anomaly_folder, filename)) for filename in os.listdir(anomaly_folder)])\n",
    "anomaly_spectrograms = preprocess_waveforms_to_spectrograms(anomaly_waveforms)\n",
    "\n",
    "anomaly_train_data, anomaly_val_data = train_test_split(anomaly_spectrograms, test_size=0.5, random_state=42)\n",
    "print(f\"Training with {anomaly_train_data.shape[0]} training samples and {anomaly_val_data.shape[0]} validation samples \")\n",
    "\n",
    "anomaly_train_spectrograms = torch.nn.functional.interpolate(anomaly_train_data.unsqueeze(1), size=(256, 31))\n",
    "anomaly_train_loader = DataLoader(TensorDataset(anomaly_train_spectrograms), batch_size=64, shuffle=True)\n",
    "\n",
    "anomaly_val_spectrograms = torch.nn.functional.interpolate(anomaly_val_data.unsqueeze(1), size=(256, 31))\n",
    "anomaly_val_loader = DataLoader(TensorDataset(anomaly_val_spectrograms), batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeeperAutoencoderWithBottleneck(d=0.08).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.005\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, threshold=0.005\n",
    ")\n",
    "\n",
    "\n",
    "def weakly_supervised_loss(model, x_norm, x_anom, margin=0.05):\n",
    "    recon_norm = model(x_norm)\n",
    "    recon_anom = model(x_anom)\n",
    "\n",
    "    L_norm = mse_loss(recon_norm, x_norm)\n",
    "    L_anom = mse_loss(recon_anom, x_anom)\n",
    "\n",
    "    separation = torch.relu(margin - (L_anom - L_norm))\n",
    "    total_loss = L_norm + separation\n",
    "\n",
    "    return total_loss, L_norm.item(), L_anom.item()\n",
    "\n",
    "\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "num_epochs = 10\n",
    "margin = 0.05\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_anomaly_losses = []\n",
    "epoch_noise_losses = []\n",
    "\n",
    "val_epoch_losses = []\n",
    "val_epoch_anomaly_losses = []\n",
    "val_epoch_noise_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    noise_losses = []\n",
    "    anomaly_losses = []\n",
    "    for (batch_noise, batch_anomaly) in zip(noise_train_loader, anomaly_train_loader):\n",
    "        x = batch_noise[0].to(device)             # unsupervised input\n",
    "        anomaly_x = batch_anomaly[0].to(device)          # weakly supervised inputs\n",
    "        optimizer.zero_grad()\n",
    "        loss, noise_loss, anomaly_loss = weakly_supervised_loss(model, x, anomaly_x, margin=margin)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        noise_losses.append(noise_loss)\n",
    "        anomaly_losses.append(anomaly_loss)\n",
    "\n",
    "    epoch_losses.append(np.mean(losses))\n",
    "    epoch_anomaly_losses.append(np.mean(anomaly_losses))\n",
    "    epoch_noise_losses.append(np.mean(noise_losses))\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_noise_losses = []\n",
    "    val_anomaly_losses = []\n",
    "    for (val_batch_noise, val_batch_anomaly) in zip(noise_val_loader, anomaly_val_loader):\n",
    "        val_x = val_batch_noise[0].to(device)             # unsupervised input\n",
    "        val_anomaly_x = val_batch_anomaly[0].to(device)          # weakly supervised inputs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_noise_loss, val_anomaly_loss = weakly_supervised_loss(model, val_x, val_anomaly_x, margin=margin)\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_noise_losses.append(val_noise_loss)\n",
    "        val_anomaly_losses.append(val_anomaly_loss)\n",
    "\n",
    "    val_epoch_losses.append(np.mean(val_losses))\n",
    "    val_epoch_anomaly_losses.append(np.mean(val_anomaly_losses))\n",
    "    val_epoch_noise_losses.append(np.mean(val_noise_losses))\n",
    "\n",
    "    scheduler.step(np.mean(val_losses))\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {loss:.6f}, Val Loss: {val_loss:.6f}, noise train/val: {noise_loss:.6f}/{val_noise_loss:.6f}, anomaly train/val: {anomaly_loss:.6f}/{val_anomaly_loss:.6f}\")\n",
    "\n",
    "\n",
    "# Save Model\n",
    "model_path = \"./models/v3.0__weakly_supervised__ET_model.pt\" #path to where the model will be saved\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee3988",
   "metadata": {},
   "source": [
    "!pip i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
